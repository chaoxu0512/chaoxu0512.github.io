---
layout:     post
title:      理解HarDNet
subtitle:   
date:       2020-09-18
author:     Chao Xu
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Image segmentation
---

# 一、初识HarDNet

在一部分，我首先简单展示一下[HarDNet](https://arxiv.org/abs/1909.00948)的效果， 它使用了经过ImageNet预训练的权重。在这里采用的是[PyTorch](https://pytorch.org/)框架，在[Google Colab](https://colab.research.google.com/)上测试运行。

## 1.1 介绍HarDNet

HarDNet指的是Harmonic DenseNet: A low memory traffic network，其突出的特点就是低内存占用率。过去几年，随着更强的计算能力和更大的数据集，我们能够训练更加复杂的网络。对于实时应用，我们面临的问题是如何在提高计算效率的同时，降低功耗。在这种情况下，作者们提出了HarDNet在两者之间寻求最佳平衡。

## 1.2 HarDNet的架构

 HarDNet可以用于图像分割和目标检测，其架构是基于Densely Connected Network。在HarDNet中，作者提出了Harmonic Dense Bocks的概念。如图1所示，可以看到该网络就像多个谐波。HarDNet的全称就是Harmonic Densely Connected Network。

<p align="center">
  <img src="https://i.loli.net/2020/09/18/h5uwXiOLBefC68p.png">
  <br>
  <b>图1. Illustrations for HarDNet</b><br>
</p>

## 1.3 HarDNet的可能应用

我们有许多图像分类算法，但是与其他分类算法相比，HarDNet可以降低功耗并达到类似的精度。 

在目标检测中，SSD使用HarDNet-68作为最先进的骨干网络；在图像分割中，可以使用HarDNet对图像进行下采样。

# 二、再看HarDNet

## 2.1 背景

在神经网络的推理阶段，如何增加计算效率，并减小功耗，这是一个关键问题。

过去是怎么做的呢？首先就是减小模型的尺寸（模型的参数量和权重），这就意味着更小的MACs（number of multiply-accumulate operations or floating point operations）和更少的动态随机访问存储器（DRAM），这些主要用来读写模型参数和特征图。主要的工作有：Residual Networks 、SqueezeNets、Densely Connected Networks等。也可以通过pruning和quantization来减小模型尺寸和功耗。

对于语义分割任务，中间特征图的总尺寸是模型尺寸的几百倍，这样就会使DRAM过度存取特征图，导致推理速度变慢。

目前减小特征图尺寸的方法基本都是有损的（比如，subsampling），这样会使准确率下降。本文作者设计了一种CNN结构，减小了特征图的DRAM存取量，同时不损害准确率。

## 2.2 评价指标

Nvidia profiler：DRAM读/写的字节数。

ARM Scale Sim：每个CNN框架的流量数据和推理次数。

Convolutional Input/Output (CIO)：每个卷积层的输入和输出尺寸之和。COI是DRAM流量的近似处理。
$$
C I O=\sum_{l}\left(c_{i n}^{(l)} \times w_{i n}^{(l)} \times h_{i n}^{(l)}+c_{o u t}^{(l)} \times w_{o u t}^{(l)} \times h_{o u t}^{(l)}\right)
$$
MoC：MACs/COO。在MoC低于某个值时，某个层的延时对应于某个固定的时间。

## 2.3 基本方法

文中，作者对每一层的MoC施加一个软约束，以设计一个低CIO网络模型，并合理增加MACs。如图2所示，避免使用MoC非常低的层，例如具有非常大输入/输出通道比的Conv1x1层。受Densely Connected Networks的启发，作者提出了Harmonic Densely Connected Network (HarD- Net) 。首先减少来自DenseNet的大部分层连接，以降低级联损耗。然后，通过增加层的通道宽度来平衡输入/输出通道比率。

<p align="center">
  <img src="https://i.loli.net/2020/09/18/OKqyucE5BiXm3Ia.png">
  <br>
  <b>图2. Concept of MoC constraint</b><br>
</p>
> 我们想要的是：较高的MoC;较小的CIO；适中的MACs。

## 2.4 结论

HarDNets reduces DRAM traffic by 40% compared with DenseNets.

Compared to DenseNet and ResNet, HarDNet achieves the same accuracy with 30%∼50% less CIO, and accordingly, 30%∼40% less inference time.

# 三、深入HarDNet

## 3.1  稀疏化和权重化

文中采用的稀疏连接方式：当$k$能被$2^n$整除，让$k$层和$k-2^n$层相连，其中$n$为非负整数；并且还需满足$k-2^{n} \ge 0$。

如果k=0，则0层就是输入层。

如果k=1，则1层可以与0层（n=0）相连；

如果k=2，则2层可以与1层（n=0）、0层（n=1）相连；

如果k=3，则3层可以与2层（n=0）相连；

如果k=4，则4层可以与3层（n=0）、2层（n=1）、0层（n=2）相连；

如果k=5，则5层可以与4层（n=0）相连；

如果k=6,，则6层可以与5层（n=0）、4层（n=1）相连；

## 3.2 Transition and Bottleneck Layers

将上面提到的各种层组合起来，就构成了一个Harmonic Dense Block (HDB)。在HDB后连接一个1x1 conv层，作为trainsition。此外，作者设置HDB的深度为$L=2^n$，这样一个HDB的最后一层就有最大的通道数，梯度最多能传输$\text{log}L$层。为了缓解这种梯度消失，作者将一个HDB的输出设置为第L层和它前面所有奇数层的级联。当完成HDB以后，就可以丢弃从2至L-2的所有偶数层。当m=1.6-1.9时，这些偶数层的内存占用是奇数层的2至3倍。

## 3.3 详细设计

如下图所示，作者提出了6个版本的HarDNet，在transition layer中设置reduction rate为0.85，因为已经对growth rate multiplier设置了一个低维的压缩因子。为了使深度具有多样性，作者还将一个block分成多个blocks，一共有16层（当考虑bottleneck layers时则为20层）。

<p align="center">
  <img src="https://i.loli.net/2020/09/21/uetzToyJhfZB9WG.png" width="540" title="Detailed implementation parameters">
  <br>
  <b>图3. Detailed implementation parameters. A “3x3, 64” stands for a Conv3x3 layer with 64 output channels, and the leading numbers below Stride 2 stand for an HDB with how many layers, followed by its growth rate k and a transitional Conv1x1 with t output channels.</b><br>
</p>

进一步，作者提出了HarDNet-68。去除了全局稠密连接；使用了MaxPool进行下采样；采用Conv-BN-ReLU的顺序；每个HDB中的growth rate k 能够增强CIO的效率。因为一个较深的HDB具有更大的输入通道数，所以一个更大的growth rate能够促进某一层输入和输出之间的通道比，从而达到MoC的限制。

此外，作者还提出了HarDNet-39DS，采用了完全的深度可分离卷积（除了第一个卷积层）。如图4b所示，作者将一个3x3 Conv层分解为一个逐点卷积和一个逐深度卷积。需要注意的是，这里逐点和逐深度卷积的顺序不能改变。因为一个HDB中每层均有一个宽输入和一个窄输出，若改变卷积顺序的话，会极大增加COI。

<p align="center">
  <img src="https://i.loli.net/2020/09/21/yiCamSOxVD68dGo.png" width="540">
  <br>
  <b>图4. (a) Inverted transition down module, (b)Depthwise-separable convolution for HarDNet.</b><br>
</p>



# Reference

[1]. [HarDNet: A Low Memory Traffic Network](https://arxiv.org/abs/1909.0094)

[2]. [Building your own Object Recognition in Pytorch – A Guide to Implement HarDNet in PyTorch](https://analyticsindiamag.com/building-your-own-object-recognition-in-pytorch-a-guide-to-implement-hardnet-in-pytorch/)
