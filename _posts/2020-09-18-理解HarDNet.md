---
layout:     post
title:      理解HarDNet
subtitle:   
date:       2020-09-18
author:     Chao Xu
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - Image segmentation
---

# 一、初识HarDNet

在一部分，我首先简单展示一下[HarDNet](https://arxiv.org/abs/1909.00948)的效果， 它使用了经过ImageNet预训练的权重。在这里采用的是[PyTorch](https://pytorch.org/)框架，在[Google Colab](https://colab.research.google.com/)上测试运行。

## 1.1 介绍HarDNet

HarDNet指的是Harmonic DenseNet: A low memory traffic network，其突出的特点就是低内存占用率。过去几年，随着更强的计算能力和更大的数据集，我们能够训练更加复杂的网络。对于实时应用，我们面临的问题是如何在提高计算效率的同时，降低功耗。在这种情况下，作者们提出了HarDNet在两者之间寻求最佳平衡。

## 1.2 HarDNet的架构

 HarDNet可以用于图像分割和目标检测，其架构是基于Densely Connected Network。在HarDNet中，作者提出了Harmonic Dense Bocks的概念。如图1所示，可以看到该网络就像多个谐波。HarDNet的全称就是Harmonic Densely Connected Network。

<p align="center">
    ![image.png](https://i.loli.net/2020/09/18/h5uwXiOLBefC68p.png)

    <div style="text-align:center">图1. Illustrations for HarDNet<div>
</p>

## 1.3 HarDNet的可能应用

我们有许多图像分类算法，但是与其他分类算法相比，HarDNet可以降低功耗并达到类似的精度。 

在目标检测中，SSD使用HarDNet-68作为最先进的骨干网络；在图像分割中，可以使用HarDNet对图像进行下采样。

# 二、再看HarDNet

## 2.1 背景

在神经网络的推理阶段，如何增加计算效率，并减小功耗，这是一个关键问题。

过去是怎么做的呢？首先就是减小模型的尺寸（模型的参数量和权重），这就意味着更小的MACs（number of multiply-accumulate operations or floating point operations）和更少的动态随机访问存储器（DRAM），这些主要用来读写模型参数和特征图。主要的工作有：Residual Networks 、SqueezeNets、Densely Connected Networks等。也可以通过pruning和quantization来减小模型尺寸和功耗。

对于语义分割任务，中间特征图的总尺寸是模型尺寸的几百倍，这样就会使DRAM过度存取特征图，导致推理速度变慢。

目前减小特征图尺寸的方法基本都是有损的（比如，subsampling），这样会使准确率下降。本文作者设计了一种CNN结构，减小了特征图的DRAM存取量，同时不损害准确率。

## 2.2 评价指标

Nvidia profiler：DRAM读/写的字节数。

ARM Scale Sim：每个CNN框架的流量数据和推理次数。

Convolutional Input/Output (CIO)：每个卷积层的输入和输出尺寸之和。COI是DRAM流量的近似处理。
$$
C I O=\sum_{l}\left(c_{i n}^{(l)} \times w_{i n}^{(l)} \times h_{i n}^{(l)}+c_{o u t}^{(l)} \times w_{o u t}^{(l)} \times h_{o u t}^{(l)}\right)
$$
MoC：MACs/COO。在MoC低于某个值时，某个层的延时对应于某个固定的时间。

## 2.3 基本方法

文中，作者对每一层的MoC施加一个软约束，以设计一个低CIO网络模型，并合理增加MACs。如图2所示，避免使用MoC非常低的层，例如具有非常大输入/输出通道比的Conv1x1层。受Densely Connected Networks的启发，作者提出了Harmonic Densely Connected Network (HarD- Net) 。首先减少来自DenseNet的大部分层连接，以降低级联损耗。然后，通过增加层的通道宽度来平衡输入/输出通道比率。

<p align="center">
![](https://i.loli.net/2020/09/18/OKqyucE5BiXm3Ia.png)
</p>

<div style="text-align:center">图2. Concept of MoC constraint<div>

<!--我们想要的是：较高的MoC;较小的CIO；适中的MACs。-->

## 2.4 结论

HarDNets reduces DRAM traffic by 40% compared with DenseNets.

Compared to DenseNet and ResNet, HarDNet achieves the same accuracy with 30%∼50% less CIO, and accordingly, 30%∼40% less inference time.

# 三、深入HarDNet

## 3.1  稀疏化和权重化

文中采用的稀疏连接方式：当$k$能被$2^n$整除，让$k$层和$k-2^n$层相连，其中$n$为非负整数；并且还需满足$k-2^{n} \ge 0$。

如果k=0，则0层就是输入层。

如果k=1，则1层可以与0层（n=0）相连；

如果k=2，则2层可以与1层（n=0）、0层（n=1）相连；

如果k=3，则3层可以与2层（n=0）相连；

如果k=4，则4层可以与3层（n=0）、2层（n=1）、0层（n=2）相连；

如果k=5，则5层可以与4层（n=0）相连；

如果k=6,，则6层可以与5层（n=0）、4层（n=1）相连；

# Reference

[1]. [HarDNet: A Low Memory Traffic Network](https://arxiv.org/abs/1909.0094)

[2]. [Building your own Object Recognition in Pytorch – A Guide to Implement HarDNet in PyTorch](https://analyticsindiamag.com/building-your-own-object-recognition-in-pytorch-a-guide-to-implement-hardnet-in-pytorch/)

